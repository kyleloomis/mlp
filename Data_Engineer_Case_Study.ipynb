{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:15:06.080923Z",
     "start_time": "2025-03-23T20:15:04.862248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install required libraries\n",
    "!pip install pandas numpy requests pydantic lxml pymupdf mistralai fastapi pytest pytest-asyncio pandas httpx uvicorn openpyxl pillow nest_asyncio"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv/lib/python3.10/site-packages (2.2.3)\r\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (2.2.4)\r\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (2.32.3)\r\n",
      "Requirement already satisfied: pydantic in ./venv/lib/python3.10/site-packages (2.10.6)\r\n",
      "Requirement already satisfied: lxml in ./venv/lib/python3.10/site-packages (5.3.1)\r\n",
      "Requirement already satisfied: pymupdf in ./venv/lib/python3.10/site-packages (1.25.4)\r\n",
      "Requirement already satisfied: mistralai in ./venv/lib/python3.10/site-packages (1.5.1)\r\n",
      "Requirement already satisfied: fastapi in ./venv/lib/python3.10/site-packages (0.115.11)\r\n",
      "Requirement already satisfied: pytest in ./venv/lib/python3.10/site-packages (8.3.5)\r\n",
      "Requirement already satisfied: pytest-asyncio in ./venv/lib/python3.10/site-packages (0.25.3)\r\n",
      "Requirement already satisfied: httpx in ./venv/lib/python3.10/site-packages (0.28.1)\r\n",
      "Requirement already satisfied: uvicorn in ./venv/lib/python3.10/site-packages (0.34.0)\r\n",
      "Requirement already satisfied: openpyxl in ./venv/lib/python3.10/site-packages (3.1.5)\r\n",
      "Requirement already satisfied: pillow in ./venv/lib/python3.10/site-packages (11.1.0)\r\n",
      "Requirement already satisfied: nest_asyncio in ./venv/lib/python3.10/site-packages (1.6.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.10/site-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.10/site-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests) (2025.1.31)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.10/site-packages (from pydantic) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./venv/lib/python3.10/site-packages (from pydantic) (2.27.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./venv/lib/python3.10/site-packages (from pydantic) (4.12.2)\r\n",
      "Requirement already satisfied: eval-type-backport>=0.2.0 in ./venv/lib/python3.10/site-packages (from mistralai) (0.2.2)\r\n",
      "Requirement already satisfied: jsonpath-python>=1.0.6 in ./venv/lib/python3.10/site-packages (from mistralai) (1.0.6)\r\n",
      "Requirement already satisfied: typing-inspect>=0.9.0 in ./venv/lib/python3.10/site-packages (from mistralai) (0.9.0)\r\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in ./venv/lib/python3.10/site-packages (from fastapi) (0.46.1)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in ./venv/lib/python3.10/site-packages (from pytest) (1.2.2)\r\n",
      "Requirement already satisfied: iniconfig in ./venv/lib/python3.10/site-packages (from pytest) (2.0.0)\r\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from pytest) (24.2)\r\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in ./venv/lib/python3.10/site-packages (from pytest) (1.5.0)\r\n",
      "Requirement already satisfied: tomli>=1 in ./venv/lib/python3.10/site-packages (from pytest) (2.2.1)\r\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.10/site-packages (from httpx) (4.8.0)\r\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.10/site-packages (from httpx) (1.0.7)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.10/site-packages (from httpcore==1.*->httpx) (0.14.0)\r\n",
      "Requirement already satisfied: click>=7.0 in ./venv/lib/python3.10/site-packages (from uvicorn) (8.1.8)\r\n",
      "Requirement already satisfied: et-xmlfile in ./venv/lib/python3.10/site-packages (from openpyxl) (2.0.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.10/site-packages (from anyio->httpx) (1.3.1)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./venv/lib/python3.10/site-packages (from typing-inspect>=0.9.0->mistralai) (1.0.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Background Information: SEC ADV Filings\n",
    "\n",
    "The SEC Form ADV is a key regulatory document used by investment advisers to register with the U.S. Securities and Exchange Commission (SEC). It serves as a comprehensive disclosure form that provides detailed information about an investment adviser’s business, including its operations, services, and any potential conflicts of interest.\n",
    "\n",
    "**Key Components of the SEC Form ADV:**\n",
    "\n",
    "1. **Part 1A:** This section contains information about the adviser's business, ownership, clients, employees, business practices, affiliations, and any disciplinary events of the adviser or its employees. It also includes details about the private funds managed by the adviser.\n",
    "\n",
    "2. **Part 2A (Brochure):** This section provides a narrative description of the adviser’s business practices, fees, conflicts of interest, and disciplinary information. It is designed to be a plain-English disclosure document that is given to clients.\n",
    "\n",
    "3. **Part 2B (Brochure Supplement):** This section includes information about the advisory personnel on whom clients rely for investment advice, including their education, business experience, and any disciplinary history.\n",
    "\n",
    "**Why is the SEC Form ADV Important?**\n",
    "\n",
    "- **Transparency:** The form promotes transparency by providing clients and regulators with essential information about the adviser’s business practices and any potential conflicts of interest.\n",
    "- **Regulatory Compliance:** Filing the Form ADV is a regulatory requirement for investment advisers, ensuring they operate within the legal framework set by the SEC and state authorities.\n",
    "- **Investor Protection:** By disclosing detailed information about their operations and practices, advisers help protect investors from fraud and misrepresentation.\n",
    "\n",
    "**Private Fund Reporting:**\n",
    "\n",
    "Within the SEC Form ADV, Sections 7.B.(1) and 7.B.(2) are specifically focused on private fund reporting. These sections require advisers to provide detailed information about the private funds they manage or advise, including the fund’s name, type, gross asset value, and regulatory status. This information helps regulators and investors understand the scope and nature of the adviser’s involvement with private funds.\n",
    "\n",
    "**Task Context:**\n",
    "\n",
    "For this task, you will be working with data extracted from SEC ADV filings. You will download metadata and PDFs for specific firms, extract relevant information, and analyze it to identify top-performing funds. This exercise will help you understand how regulatory filings can be used to gather critical information about investment advisers and their managed funds."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Code"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T19:56:07.181482Z",
     "start_time": "2025-03-23T19:56:07.055411Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 7,
   "source": [
    "from typing import List\n",
    "\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "\n",
    "class RunConfiguration(BaseModel):\n",
    "    \"\"\"\n",
    "    Configuration class for MLP pipeline runs. Uses Pydantic data class to enable JSON\n",
    "    serialization/deserialization of configuration.\n",
    "    \"\"\"\n",
    "    firm_crd_numbers: List[int]\n",
    "    input_dir: str\n",
    "    output_dir: str\n",
    "    db_path: str\n",
    "    raise_error: bool = Field(default=False, description=\"Raise error upon failure\")\n",
    "    verbose: bool = Field(default=False, description=\"Enable verbose logging\")\n",
    "\n",
    "    @field_validator('firm_crd_numbers')\n",
    "    def check_firm_crd_numbers(cls, v):\n",
    "        if not v:\n",
    "            raise ValueError(\"At least one firm CRD number must be provided\")\n",
    "        return v\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T19:56:12.945314Z",
     "start_time": "2025-03-23T19:56:12.942246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "from abc import ABC\n",
    "\n",
    "\n",
    "class BaseSource(ABC):\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T19:56:22.222813Z",
     "start_time": "2025-03-23T19:56:22.206655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Data source for SEC ADV metadata\n",
    "\"\"\"\n",
    "\n",
    "import gzip\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "class Source(BaseSource):\n",
    "    \"\"\"Data source for SEC ADV metadata\"\"\"\n",
    "\n",
    "    def __init__(self, config: RunConfiguration):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "    def run(self, as_of_date: datetime, firm_crd_numbers: List[int]) -> Dict[int, str]:\n",
    "        df_metadata = self.download_metadata(as_of_date)\n",
    "        return self.download_pdf(df_metadata, firm_crd_numbers)\n",
    "\n",
    "    def download_metadata(self, as_of_date: datetime) -> pd.DataFrame:\n",
    "        \"\"\"Download metadata from SEC\"\"\"\n",
    "        as_of_date_str = as_of_date.strftime('%m_%d_%Y')\n",
    "        self.logger.info(f\"Downloading metadata for {as_of_date_str}\")\n",
    "\n",
    "        path = rf\"https://reports.adviserinfo.sec.gov/reports/CompilationReports/IA_FIRM_SEC_Feed_{as_of_date_str}.xml.gz\"\n",
    "\n",
    "        try:\n",
    "            response = requests.get(path)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                # Unzip the content\n",
    "                with gzip.GzipFile(fileobj=io.BytesIO(response.content)) as gz:\n",
    "                    # Ensure the content is read as a string with the correct encoding\n",
    "                    xml_content = gz.read().decode('ISO-8859-1')\n",
    "\n",
    "                # Read the data into a dataframe specifying the encoding explicitly if needed\n",
    "                df = pd.read_xml(io.StringIO(xml_content), xpath='//Info', encoding='ISO-8859-1')\n",
    "\n",
    "                # Apply metadata transformer\n",
    "                return self._transform_metadata(df)\n",
    "            else:\n",
    "                self.logger.error(f\"Failed to download metadata. Status code: {response.status_code}\")\n",
    "                if self.config.raise_error:\n",
    "                    raise Exception(f\"Failed to download metadata. Status code: {response.status_code}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error downloading metadata: {str(e)}\")\n",
    "            if self.config.raise_error:\n",
    "                raise\n",
    "\n",
    "    def _transform_metadata(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Transform SEC ADV metadata\"\"\"\n",
    "        self.logger.info(\"Transforming SEC ADV metadata...\")\n",
    "\n",
    "        # Create a column with paths for the PDFs\n",
    "        data['DownloadPath'] = data['FirmCrdNb'].apply(\n",
    "            lambda x: f\"https://reports.adviserinfo.sec.gov/reports/ADV/{x}/PDF/{x}.pdf\"\n",
    "        )\n",
    "        data.set_index('FirmCrdNb', inplace=True)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def download_pdf(self, df_metadata: pd.DataFrame, firm_crd_numbers: List[int]) -> Dict[int, str]:\n",
    "        \"\"\"Download PDF files for the specified firm CRD numbers\"\"\"\n",
    "        self.logger.info(\"Downloading PDF files...\")\n",
    "        result_paths = {}\n",
    "\n",
    "        for firm_crd in firm_crd_numbers:\n",
    "            try:\n",
    "                # Validate the firm CRD exists in metadata\n",
    "                if firm_crd not in df_metadata.index:\n",
    "                    self.logger.warning(f\"Firm CRD {firm_crd} not found in metadata\")\n",
    "                    continue\n",
    "\n",
    "                # Prepare file paths\n",
    "                url = df_metadata.loc[firm_crd]['DownloadPath']\n",
    "                file_name = url.split('/')[-1]\n",
    "                save_path = os.path.join(self.config.input_dir, file_name)\n",
    "\n",
    "                # Check if the file already exists\n",
    "                if os.path.isfile(save_path):\n",
    "                    self.logger.info(f\"File {file_name} already exists. Using existing file.\")\n",
    "                    result_paths[firm_crd] = save_path\n",
    "                else:\n",
    "                    # Download the file\n",
    "                    if self._download_file(url, save_path):\n",
    "                        result_paths[firm_crd] = save_path\n",
    "                    elif self.config.raise_error:\n",
    "                        self.logger.error(f\"Failed to download file for Firm CRD {firm_crd}\")\n",
    "                        raise Exception(f\"Failed to download file for Firm CRD {firm_crd}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing Firm CRD {firm_crd}: {str(e)}\")\n",
    "                if self.config.raise_error:\n",
    "                    raise\n",
    "\n",
    "        return result_paths\n",
    "\n",
    "    def _download_file(self, url: str, save_path: str) -> bool:\n",
    "        \"\"\"Download a file from a URL and save it locally\"\"\"\n",
    "        try:\n",
    "            # Request the file\n",
    "            response = requests.get(url)\n",
    "\n",
    "            # Check response status\n",
    "            if response.status_code == 200:\n",
    "                # Write file to disk\n",
    "                with open(save_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                self.logger.info(f\"Downloaded {save_path}\")\n",
    "                return True\n",
    "            else:\n",
    "                self.logger.error(f\"Failed to download from {url}. Status code: {response.status_code}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error downloading {url}: {str(e)}\")\n",
    "            return False\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T19:58:30.777486Z",
     "start_time": "2025-03-23T19:58:30.771671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Base transformer class for MLP pipeline\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class BaseTransformer(ABC):\n",
    "    \"\"\"Base transformer class for MLP pipeline\"\"\"\n",
    "\n",
    "    def __init__(self, config: RunConfiguration):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, data: Any) -> Any:\n",
    "        \"\"\"Transform the input data\"\"\"\n",
    "        pass\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:00:06.254443Z",
     "start_time": "2025-03-23T20:00:06.250297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "from abc import abstractmethod\n",
    "\n",
    "\n",
    "class BaseOCRReader:\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    @abstractmethod\n",
    "    def read(self, pdf_path: str):\n",
    "        pass\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:00:06.880261Z",
     "start_time": "2025-03-23T20:00:06.876241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "import fitz\n",
    "\n",
    "\n",
    "class FitzOCRReader(BaseOCRReader):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def read(self, pdf_path: str):\n",
    "        doc = fitz.open(pdf_path)\n",
    "        full_text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
    "        return full_text\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:00:43.216950Z",
     "start_time": "2025-03-23T20:00:43.181616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import re\n",
    "from typing import Dict, Optional, Any, List\n",
    "\n",
    "import fitz\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PdfTransformer(BaseTransformer):\n",
    "    \"\"\"Transform extracted text from SEC ADV PDF files\"\"\"\n",
    "\n",
    "    def __init__(self, config: RunConfiguration, ocr_reader: BaseOCRReader = FitzOCRReader()):\n",
    "        super().__init__(config)\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.ocr_reader = ocr_reader\n",
    "\n",
    "    def transform(self, firm_crd_number: int) -> Dict[str, Any]:\n",
    "        \"\"\"Extract text from a PDF file and parse relevant information\"\"\"\n",
    "        pdf_path = f\"{self.config.input_dir}/{firm_crd_number}.pdf\"\n",
    "        self.logger.info(f\"Transforming PDF content at {pdf_path}...\")\n",
    "\n",
    "        full_text = self.ocr_reader.read(pdf_path)\n",
    "\n",
    "        return {\n",
    "            'firm_crd_nb': self._extract_firm_crd_number(full_text),\n",
    "            'sec_nb': self._extract_sec_number(full_text),\n",
    "            'business_name': self._extract_business_name(full_text),\n",
    "            'full_legal_name': self._extract_full_legal_name(full_text),\n",
    "            'address': self._extract_address(full_text),\n",
    "            'phone_number': self._extract_phone_number(full_text),\n",
    "            'compensation_arrangements': self._extract_compensation_arrangements(pdf_path),\n",
    "            'employee_count': self._extract_investment_advisory_employee_count(full_text),\n",
    "            'client_types': self._extract_client_types(full_text),\n",
    "            'private_funds': self._extract_private_funds_and_ids(full_text),\n",
    "            'signatory': self._extract_signatory(full_text)\n",
    "        }\n",
    "\n",
    "    def _extract_with_regex(self, text: str, pattern: str) -> Optional[str]:\n",
    "        \"\"\"Extract information using regex and return the first capturing group\"\"\"\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        self.logger.debug(f\"No match found for pattern: {pattern}\")\n",
    "        return None\n",
    "\n",
    "    def _extract_firm_crd_number(self, text: str) -> Optional[int]:\n",
    "        \"\"\"Extract firm CRD number\"\"\"\n",
    "        patterns = [\n",
    "            r'CRD Number:\\s*(\\d+)',\n",
    "            r'your CRD number:\\s*(\\d+)',\n",
    "            r'CRD number:\\s*(\\d+)',\n",
    "            r'CRD Number\\D*(\\d+)'\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            result = self._extract_with_regex(text, pattern)\n",
    "            if result:\n",
    "                try:\n",
    "                    return int(result)\n",
    "                except ValueError:\n",
    "                    self.logger.warning(\"Failed to convert extracted firm CRD number to int.\")\n",
    "                    return None\n",
    "\n",
    "        self.logger.warning(\"Failed to extract Firm CRD Number.\")\n",
    "        return None\n",
    "\n",
    "    def _extract_sec_number(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract SEC file number\"\"\"\n",
    "        patterns = [\n",
    "            r'SEC file number:\\s*([\\d-]+)',\n",
    "            r'SEC File Number:\\s*([\\d-]+)',\n",
    "            r'you are registered with the SEC as an investment adviser, your SEC file number:\\s*([\\d-]+)',\n",
    "            r'your SEC file number:\\s*([\\d-]+)'\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            result = self._extract_with_regex(text, pattern)\n",
    "            if result:\n",
    "                return result\n",
    "\n",
    "        self.logger.warning(\"Failed to extract SEC Number.\")\n",
    "        return None\n",
    "\n",
    "    def _extract_business_name(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract primary business name\"\"\"\n",
    "        patterns = [\n",
    "            r'Primary Business Name:\\s*([^\\n]+)',\n",
    "            r'Name under which you primarily conduct your advisory business[^:]*:\\s*([^\\n]+)'\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            result = self._extract_with_regex(text, pattern)\n",
    "            if result:\n",
    "                return result\n",
    "\n",
    "        self.logger.warning(\"Failed to extract Business Name.\")\n",
    "        return None\n",
    "\n",
    "    def _extract_full_legal_name(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract full legal name\"\"\"\n",
    "        patterns = [\n",
    "            r'Your full legal name.*?:\\s*([^\\n]+)',\n",
    "            r'full legal name.*?:\\s*([^\\n]+)',\n",
    "            r'A\\.\\s+Your full legal name.*?:\\s*([^\\n]+)'\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            result = self._extract_with_regex(text, pattern)\n",
    "            if result:\n",
    "                return result\n",
    "\n",
    "        self.logger.warning(\"Failed to extract Full Legal Name.\")\n",
    "        return None\n",
    "\n",
    "    def _extract_phone_number(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract phone number\"\"\"\n",
    "        patterns = [\n",
    "            r'Telephone Number:\\s*([\\d\\- \\(\\)\\+]+)',\n",
    "            r'Telephone number at this location:\\s*([\\d\\- \\(\\)\\+]+)'\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            result = self._extract_with_regex(text, pattern)\n",
    "            if result:\n",
    "                return result\n",
    "\n",
    "        self.logger.warning(\"Failed to extract Phone Number.\")\n",
    "        return None\n",
    "\n",
    "    def _extract_investment_advisory_employee_count(self, text: str) -> Optional[int]:\n",
    "        \"\"\"Extract the number of employees performing investment advisory functions\"\"\"\n",
    "        pattern = r'Approximately how many of the employees reported in 5\\.A\\. perform investment advisory functions.*?\\n(\\d+)'\n",
    "        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "        if match:\n",
    "            try:\n",
    "                return int(match.group(1).strip())\n",
    "            except ValueError:\n",
    "                self.logger.warning(\"Failed to convert extracted employee count to int.\")\n",
    "                return None\n",
    "\n",
    "        self.logger.warning(\"Investment advisory employee count not found.\")\n",
    "        return None\n",
    "\n",
    "    def _extract_address(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract and format address from text input\"\"\"\n",
    "        components = {}\n",
    "        patterns = {\n",
    "            'street1': r'Number and Street 1:\\s*(.+?)(?:\\s*Number and Street 2:|$)',\n",
    "            'street2': r'Number and Street 2:\\s*(.+?)(?:\\s*City:|$)',\n",
    "            'city': r'City:\\s*(.+?)(?:\\s*State:|$)',\n",
    "            'state': r'State:\\s*(.+?)(?:\\s*Country:|$)',\n",
    "            'country': r'Country:\\s*(.+?)(?:\\s*ZIP\\+4/Postal Code:|$)',\n",
    "            'postal_code': r'ZIP\\+4/Postal Code:\\s*(.+?)(?:\\s*If this address is|$)'\n",
    "        }\n",
    "\n",
    "        for key, pattern in patterns.items():\n",
    "            match = re.search(pattern, text, re.DOTALL)\n",
    "            components[key] = match.group(1).strip() if match and match.group(1).strip() else None\n",
    "\n",
    "        address_parts = []\n",
    "\n",
    "        street_parts = []\n",
    "        if components['street1']:\n",
    "            street_parts.append(components['street1'])\n",
    "        if components['street2']:\n",
    "            street_parts.append(components['street2'])\n",
    "        if street_parts:\n",
    "            address_parts.append(', '.join(street_parts))\n",
    "\n",
    "        if components['country'] == 'United States':\n",
    "            city_state = []\n",
    "            if components['city']:\n",
    "                city_state.append(components['city'])\n",
    "            if components['state']:\n",
    "                city_state.append(components['state'])\n",
    "            if city_state:\n",
    "                address_parts.append(', '.join(city_state))\n",
    "        elif components['city']:\n",
    "            address_parts.append(components['city'])\n",
    "\n",
    "        if components['country'] == 'United States' and components['postal_code']:\n",
    "            if address_parts and components['state']:\n",
    "                last_part = address_parts[-1]\n",
    "                address_parts[-1] = f\"{last_part} {components['postal_code']}\"\n",
    "            else:\n",
    "                address_parts.append(components['postal_code'])\n",
    "        elif components['postal_code']:\n",
    "            address_parts.append(components['postal_code'])\n",
    "\n",
    "        if components['country']:\n",
    "            address_parts.append(components['country'])\n",
    "\n",
    "        if not address_parts:\n",
    "            self.logger.warning(\"No valid address components found.\")\n",
    "            return None\n",
    "\n",
    "        return ', '.join(address_parts)\n",
    "\n",
    "    def _is_checkbox_checked(self, page, rect, threshold=150):\n",
    "        \"\"\"Determine if a checkbox is checked based on pixel intensity.\"\"\"\n",
    "        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2), clip=rect, colorspace=\"gray\")  # High-res grayscale\n",
    "        img = Image.frombytes(\"L\", [pix.width, pix.height], pix.samples)  # Convert to PIL\n",
    "        img_array = np.array(img)  # Convert to NumPy\n",
    "\n",
    "        # Determine black pixel ratio\n",
    "        return np.sum(img_array < threshold) / img_array.size > 0.12  # Adjust sensitivity if needed\n",
    "\n",
    "    def _extract_compensation_arrangements(self, pdf_path: str) -> List[str]:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        page = None\n",
    "        for page_num in range(len(doc)):\n",
    "            target_page = doc[page_num]\n",
    "            found_rects = target_page.search_for(\"Compensation Arrangements\")\n",
    "            if found_rects:\n",
    "                page = target_page\n",
    "                break  # Stop at the first occurrence\n",
    "\n",
    "        compensation_options = {\n",
    "            \"A percentage of assets under your management\": \"(1)\",\n",
    "            \"Hourly charges\": \"(2)\",\n",
    "            \"Subscription fees (for a newsletter or periodical)\": \"(3)\",\n",
    "            \"Fixed fees (other than subscription fees)\": \"(4)\",\n",
    "            \"Commissions\": \"(5)\",\n",
    "            \"Performance-based fees\": \"(6)\",\n",
    "            \"Other (specify):\": \"(7)\"\n",
    "        }\n",
    "\n",
    "        checked_options = []\n",
    "\n",
    "        for label, option_number in compensation_options.items():\n",
    "            rects = page.search_for(label)\n",
    "            if not rects:\n",
    "                continue\n",
    "\n",
    "            label_rect = rects[0]\n",
    "            checkbox_rect = fitz.Rect(label_rect.x0 - 44, label_rect.y0 + 1, label_rect.x0 - 38, label_rect.y1 - 1)\n",
    "\n",
    "            # Draw rectangles to visualize\n",
    "            # page.draw_rect(label_rect, color=(1, 0, 0), width=0.5)  # Red for labels\n",
    "            # page.draw_rect(checkbox_rect, color=(0, 1, 0), width=1.0)  # Green for checkboxes\n",
    "\n",
    "            if self._is_checkbox_checked(page, checkbox_rect):\n",
    "                checked_options.append(label)\n",
    "\n",
    "        # for debugging pixel map\n",
    "        # pix = page.get_pixmap()\n",
    "        # img = Image.open(io.BytesIO(pix.tobytes()))\n",
    "        # img.save(\"test.png\")\n",
    "\n",
    "        return checked_options\n",
    "\n",
    "    def _extract_client_types(self, text: str) -> dict:\n",
    "        \"\"\"Extract types of clients served and their AUM\"\"\"\n",
    "        section_pattern = r'Type of Client.*?under Management(.*?)(?:Item|Section|\\Z)'\n",
    "        section_match = re.search(section_pattern, text, re.DOTALL)\n",
    "\n",
    "        if not section_match:\n",
    "            section_pattern = r'Type of Client(.*?)(?:Item|\\Z)'\n",
    "            section_match = re.search(section_pattern, text, re.DOTALL)\n",
    "\n",
    "            if not section_match:\n",
    "                self.logger.warning(\"Client types section not found.\")\n",
    "                return {}\n",
    "\n",
    "        client_section = section_match.group(1)\n",
    "        aum_details = {}\n",
    "\n",
    "        client_mapping = {\n",
    "            'a': 'Individuals',\n",
    "            'b': 'High Net Worth Individuals',\n",
    "            'c': 'Banking Institutions',\n",
    "            'd': 'Investment Companies',\n",
    "            'e': 'Business Development Companies',\n",
    "            'f': 'Pooled Investment Vehicles',\n",
    "            'g': 'Pension Plans',\n",
    "            'h': 'Charitable Organizations',\n",
    "            'i': 'State Entities',\n",
    "            'j': 'Other Advisers',\n",
    "            'k': 'Insurance Companies',\n",
    "            'l': 'Sovereign Wealth Funds',\n",
    "            'm': 'Corporations',\n",
    "            'n': 'Other'\n",
    "        }\n",
    "\n",
    "        letter_sequence = list(client_mapping.keys())\n",
    "\n",
    "        for i, letter in enumerate(letter_sequence):\n",
    "            client_type = client_mapping[letter]\n",
    "            next_pattern = rf'\\({letter_sequence[i + 1]}\\)' if i < len(letter_sequence) - 1 else r'Item|\\Z'\n",
    "            client_pattern = rf'\\({letter}\\)(.*?)(?:{next_pattern})'\n",
    "            client_match = re.search(client_pattern, client_section, re.DOTALL)\n",
    "\n",
    "            if client_match:\n",
    "                line_text = client_match.group(1)\n",
    "\n",
    "                if letter == 'n' and ':' in line_text:\n",
    "                    other_desc_match = re.search(r'Other:\\s*([^\\n$]+)', line_text)\n",
    "                    if other_desc_match:\n",
    "                        other_desc = other_desc_match.group(1).strip()\n",
    "                        client_type = f\"{client_type}: {other_desc}\"\n",
    "\n",
    "                aum_value = 0\n",
    "                aum_match = re.search(r'\\$\\s*([\\d,]+)', line_text)\n",
    "                if aum_match:\n",
    "                    aum_str = aum_match.group(1).replace(',', '')\n",
    "                    if aum_str:\n",
    "                        aum_value = int(aum_str)\n",
    "\n",
    "                if aum_value > 0:\n",
    "                    aum_details[client_type] = aum_value\n",
    "\n",
    "        return aum_details\n",
    "\n",
    "    def _extract_private_funds_and_ids(self, text: str) -> dict:\n",
    "        \"\"\"Extract private fund names and their identification numbers\"\"\"\n",
    "        pattern = r\"Name of the private fund:\\s*([^\\n]+?)\\s*\\n\\s*\\(b\\) Private fund identification number:\\s*\\(include the \\\"805-\\\" prefix also\\)\\s*(805-\\d+)\"\n",
    "        matches = re.findall(pattern, text)\n",
    "\n",
    "        results = {}\n",
    "        for fund_name, fund_id in matches:\n",
    "            clean_name = fund_name.strip()\n",
    "            clean_id = fund_id.strip()\n",
    "            results[clean_name] = clean_id\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _extract_signatory(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract signatory information from the form\"\"\"\n",
    "        patterns = [\n",
    "            r'Printed Name:\\s*\\n([^\\n]+)',\n",
    "            r'Printed Name:\\s*([^:\\n]+?)(?=\\s*\\n)',\n",
    "            r'Signature:\\s*\\n([^\\n]+)',\n",
    "            r'Signature:\\s*([^:\\n]+?)(?=\\s*\\n)'\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            result = self._extract_with_regex(text, pattern)\n",
    "            if result:\n",
    "                cleaned_result = result.strip()\n",
    "                if \"Title:\" in cleaned_result:\n",
    "                    cleaned_result = cleaned_result.split(\"Title:\")[0].strip()\n",
    "\n",
    "                if cleaned_result and not cleaned_result.isspace():\n",
    "                    return cleaned_result\n",
    "\n",
    "        self.logger.warning(\"Failed to extract Signatory.\")\n",
    "        return None\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:01:08.110385Z",
     "start_time": "2025-03-23T20:01:08.097862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Fund analysis transformer for MLP pipeline\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class FundAnalysisTransformer(BaseTransformer):\n",
    "    \"\"\"Transformer for analyzing fund performance data\"\"\"\n",
    "\n",
    "    def __init__(self, config: RunConfiguration):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def transform(self, data: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Transform and analyze the joined data from database\n",
    "\n",
    "        Args:\n",
    "            data: DataFrame from query_all() with joined table data\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of DataFrames with analysis results\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting fund analysis transformation...\")\n",
    "\n",
    "        # Clean the data and prepare for analysis\n",
    "        cleaned_data = self._clean_data(data)\n",
    "\n",
    "        # Perform analyses\n",
    "        results = {}\n",
    "        results[\"top_funds\"] = self.get_top_funds(cleaned_data)\n",
    "        results[\"aum_by_client_type\"] = self.analyze_client_distribution(cleaned_data)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _clean_data(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean and prepare the data for analysis\"\"\"\n",
    "        # Make a copy to avoid modifying the original\n",
    "        df = data.copy()\n",
    "\n",
    "        # Convert AUM values to numeric\n",
    "        if 'aum_value' in df.columns:\n",
    "            df['aum_value'] = pd.to_numeric(\n",
    "                df['aum_value'].astype(str).str.replace(r'[^\\d.]', '', regex=True),\n",
    "                errors='coerce'\n",
    "            )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_top_funds(self, data: pd.DataFrame, top_n: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Identify top-performing funds based on AUM values\n",
    "\n",
    "        Args:\n",
    "            data: DataFrame with fund data\n",
    "            top_n: Number of top funds to return\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with top funds by AUM\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Identifying top {top_n} funds by AUM...\")\n",
    "\n",
    "        # Group by firm and fund\n",
    "        fund_data = data.dropna(subset=['fund_name', 'fund_id'])\n",
    "\n",
    "        # Calculate total AUM per firm\n",
    "        firm_aum = {}\n",
    "        for firm_crd in fund_data['firm_crd_nb'].unique():\n",
    "            firm_rows = data[data['firm_crd_nb'] == firm_crd]\n",
    "            firm_aum[firm_crd] = firm_rows['aum_value'].sum()\n",
    "\n",
    "        # Create fund performance DataFrame\n",
    "        fund_metrics = []\n",
    "        for (firm_crd, fund_name), group in fund_data.groupby(['firm_crd_nb', 'fund_name']):\n",
    "            fund_metrics.append({\n",
    "                'firm_crd_nb': firm_crd,\n",
    "                'business_name': group['business_name'].iloc[0],\n",
    "                'fund_name': fund_name,\n",
    "                'fund_id': group['fund_id'].iloc[0],\n",
    "                'total_firm_aum': firm_aum.get(firm_crd, 0)\n",
    "            })\n",
    "\n",
    "        # Convert to DataFrame and sort by AUM\n",
    "        performance_df = pd.DataFrame(fund_metrics)\n",
    "        return performance_df.sort_values('total_firm_aum', ascending=False).head(top_n)\n",
    "\n",
    "    def analyze_client_distribution(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Analyze how AUM is distributed across different client types\n",
    "\n",
    "        Args:\n",
    "            data: DataFrame with client type data\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with AUM by client type\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Analyzing AUM distribution by client type...\")\n",
    "\n",
    "        # Filter for rows with client type information\n",
    "        client_data = data.dropna(subset=['client_type'])\n",
    "        client_data = client_data[client_data['client_type'] != 'None']\n",
    "\n",
    "        if client_data.empty:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Group by client type and calculate total AUM\n",
    "        aum_by_type = client_data.groupby(['client_type'])['aum_value'].sum().reset_index()\n",
    "        aum_by_type = aum_by_type.sort_values('aum_value', ascending=False)\n",
    "\n",
    "        # Calculate percentage of total AUM\n",
    "        total_aum = aum_by_type['aum_value'].sum()\n",
    "        aum_by_type['percentage'] = (aum_by_type['aum_value'] / total_aum * 100).round(2)\n",
    "\n",
    "        return aum_by_type\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:01:51.037976Z",
     "start_time": "2025-03-23T20:01:51.034149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Database manager for MLP pipeline\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from abc import abstractmethod\n",
    "\n",
    "\n",
    "class DatabaseManager:\n",
    "    \"\"\"Manager for SQLite database operations\"\"\"\n",
    "\n",
    "    def __init__(self, db_path: str):\n",
    "        self.db_path = db_path\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    @abstractmethod\n",
    "    def setup(self):\n",
    "        pass\n"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:02:15.279947Z",
     "start_time": "2025-03-23T20:02:15.258765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Database manager for MLP pipeline with improved schema design\n",
    "\"\"\"\n",
    "\n",
    "import sqlite3\n",
    "from typing import Dict, Any, Union, List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class DatabaseSink(DatabaseManager):\n",
    "    \"\"\"Manager for SQLite database operations with improved schema for nested data\"\"\"\n",
    "\n",
    "    def __init__(self, config: RunConfiguration):\n",
    "        super().__init__(config.db_path)\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"Initialize the database schema with proper relationships\"\"\"\n",
    "        self.logger.info(f\"Initializing database at {self.db_path}\")\n",
    "\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Create the main firms table\n",
    "            cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS firms (\n",
    "                firm_crd_nb INTEGER PRIMARY KEY,\n",
    "                sec_nb TEXT,\n",
    "                business_name TEXT,\n",
    "                full_legal_name TEXT,\n",
    "                address TEXT,\n",
    "                phone_number TEXT,\n",
    "                employee_count INTEGER,\n",
    "                signatory TEXT,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "            ''')\n",
    "\n",
    "            # Create compensation arrangements table with FK relationship\n",
    "            cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS compensation_arrangements (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                firm_crd_nb INTEGER NOT NULL,\n",
    "                arrangement TEXT NOT NULL,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                FOREIGN KEY (firm_crd_nb) REFERENCES firms(firm_crd_nb) ON DELETE CASCADE\n",
    "            )\n",
    "            ''')\n",
    "\n",
    "            # Create client types table with FK relationship\n",
    "            cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS client_types (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                firm_crd_nb INTEGER NOT NULL,\n",
    "                client_type TEXT NOT NULL,\n",
    "                aum_value INTEGER NOT NULL DEFAULT 0,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                FOREIGN KEY (firm_crd_nb) REFERENCES firms(firm_crd_nb) ON DELETE CASCADE\n",
    "            )\n",
    "            ''')\n",
    "\n",
    "            # Create private funds table with FK relationship\n",
    "            cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS private_funds (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                firm_crd_nb INTEGER NOT NULL,\n",
    "                fund_name TEXT NOT NULL,\n",
    "                fund_id TEXT NOT NULL,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                FOREIGN KEY (firm_crd_nb) REFERENCES firms(firm_crd_nb) ON DELETE CASCADE,\n",
    "                UNIQUE(firm_crd_nb, fund_id)\n",
    "            )\n",
    "            ''')\n",
    "\n",
    "            # Create indexes for better query performance\n",
    "            cursor.execute(\n",
    "                'CREATE INDEX IF NOT EXISTS idx_compensation_firm_crd ON compensation_arrangements(firm_crd_nb)')\n",
    "            cursor.execute('CREATE INDEX IF NOT EXISTS idx_client_types_firm_crd ON client_types(firm_crd_nb)')\n",
    "            cursor.execute('CREATE INDEX IF NOT EXISTS idx_private_funds_firm_crd ON private_funds(firm_crd_nb)')\n",
    "\n",
    "            # Enable foreign keys\n",
    "            cursor.execute('PRAGMA foreign_keys = ON')\n",
    "\n",
    "            conn.commit()\n",
    "\n",
    "    def write(self, firm_data: Dict[int, Dict[str, Any]]):\n",
    "        \"\"\"Store firm data in the database with proper handling of nested fields\n",
    "\n",
    "        Args:\n",
    "            firm_data: Dictionary with firm CRD as key and firm data as value\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Storing data for {len(firm_data)} firms\")\n",
    "\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            # Enable foreign keys\n",
    "            conn.execute('PRAGMA foreign_keys = ON')\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            for firm_crd, data in firm_data.items():\n",
    "                # Start a transaction\n",
    "                cursor.execute('BEGIN TRANSACTION')\n",
    "                try:\n",
    "                    # Insert or update firm data in the firms table\n",
    "                    cursor.execute('''\n",
    "                    INSERT INTO firms\n",
    "                    (firm_crd_nb, sec_nb, business_name, full_legal_name, address, phone_number, employee_count, signatory, updated_at)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)\n",
    "                    ON CONFLICT(firm_crd_nb) DO UPDATE SET\n",
    "                        sec_nb = excluded.sec_nb,\n",
    "                        business_name = excluded.business_name,\n",
    "                        full_legal_name = excluded.full_legal_name,\n",
    "                        address = excluded.address,\n",
    "                        phone_number = excluded.phone_number,\n",
    "                        employee_count = excluded.employee_count,\n",
    "                        signatory = excluded.signatory,\n",
    "                        updated_at = CURRENT_TIMESTAMP\n",
    "                    ''', (\n",
    "                        firm_crd,\n",
    "                        data.get('sec_nb'),\n",
    "                        data.get('business_name'),\n",
    "                        data.get('full_legal_name'),\n",
    "                        data.get('address'),\n",
    "                        data.get('phone_number'),\n",
    "                        data.get('employee_count'),\n",
    "                        data.get('signatory')\n",
    "                    ))\n",
    "\n",
    "                    # Handle compensation arrangements (expects a string that can be split)\n",
    "                    self._write_compensation_arrangements(cursor, firm_crd, data.get('compensation_arrangements', ''))\n",
    "\n",
    "                    # Handle client types (expects a dictionary)\n",
    "                    self._write_client_types(cursor, firm_crd, data.get('client_types', {}))\n",
    "\n",
    "                    # Handle private funds (expects a dictionary of fund name -> fund id)\n",
    "                    self._write_private_funds(cursor, firm_crd, data.get('private_funds', {}))\n",
    "\n",
    "                    # Commit the transaction\n",
    "                    cursor.execute('COMMIT')\n",
    "                except Exception as e:\n",
    "                    # Rollback in case of error\n",
    "                    cursor.execute('ROLLBACK')\n",
    "                    self.logger.error(f\"Error storing data for firm {firm_crd}: {str(e)}\")\n",
    "                    raise e\n",
    "\n",
    "    def _write_compensation_arrangements(self, cursor: sqlite3.Cursor, firm_crd: str, arrangements: Union[str, list]):\n",
    "        \"\"\"Store compensation arrangements in the database\n",
    "\n",
    "        Args:\n",
    "            cursor: SQLite cursor\n",
    "            firm_crd: Firm CRD number\n",
    "            arrangements: List of compensation arrangements or a comma-separated string\n",
    "        \"\"\"\n",
    "        # Delete existing arrangements for this firm\n",
    "        cursor.execute('DELETE FROM compensation_arrangements WHERE firm_crd_nb = ?', (firm_crd,))\n",
    "\n",
    "        # Ensure arrangements are always a list\n",
    "        if isinstance(arrangements, list):\n",
    "            arrangement_list = [arr.strip() for arr in arrangements if isinstance(arr, str)]\n",
    "        elif isinstance(arrangements, str):\n",
    "            arrangement_list = [arr.strip() for arr in arrangements.split(',') if arr.strip()]\n",
    "        else:\n",
    "            arrangement_list = []\n",
    "\n",
    "        # Insert into DB\n",
    "        for arrangement in arrangement_list:\n",
    "            cursor.execute(\n",
    "                '''INSERT INTO compensation_arrangements (firm_crd_nb, arrangement) VALUES (?, ?)''',\n",
    "                (firm_crd, arrangement)\n",
    "            )\n",
    "\n",
    "    def _write_client_types(self, cursor: sqlite3.Cursor, firm_crd: str, client_types: Dict[str, Union[int, str]]):\n",
    "        \"\"\"Store client types and AUM values in the database\n",
    "\n",
    "        Args:\n",
    "            cursor: SQLite cursor\n",
    "            firm_crd: Firm CRD number\n",
    "            client_types: Dictionary of client type -> AUM value\n",
    "        \"\"\"\n",
    "        # Delete existing client types for this firm\n",
    "        cursor.execute('DELETE FROM client_types WHERE firm_crd_nb = ?', (firm_crd,))\n",
    "\n",
    "        # Insert new client types\n",
    "        for client_type, aum_value in client_types.items():\n",
    "            # Convert AUM value to integer if it's not already\n",
    "            if isinstance(aum_value, str):\n",
    "                try:\n",
    "                    aum_value = int(aum_value.replace(',', ''))\n",
    "                except ValueError:\n",
    "                    aum_value = 0\n",
    "\n",
    "            cursor.execute('''\n",
    "            INSERT INTO client_types (firm_crd_nb, client_type, aum_value)\n",
    "            VALUES (?, ?, ?)\n",
    "            ''', (firm_crd, client_type, aum_value))\n",
    "\n",
    "    def _write_private_funds(self, cursor: sqlite3.Cursor, firm_crd: str, private_funds: Dict[str, str]):\n",
    "        \"\"\"Store private funds in the database\n",
    "\n",
    "        Args:\n",
    "            cursor: SQLite cursor\n",
    "            firm_crd: Firm CRD number\n",
    "            private_funds: Dictionary of fund name -> fund ID\n",
    "        \"\"\"\n",
    "        # Delete existing private funds for this firm\n",
    "        cursor.execute('DELETE FROM private_funds WHERE firm_crd_nb = ?', (firm_crd,))\n",
    "\n",
    "        # Insert new private funds\n",
    "        for fund_name, fund_id in private_funds.items():\n",
    "            cursor.execute('''\n",
    "            INSERT INTO private_funds (firm_crd_nb, fund_name, fund_id)\n",
    "            VALUES (?, ?, ?)\n",
    "            ''', (firm_crd, fund_name, fund_id))\n",
    "\n",
    "    def query_all(self) -> pd.DataFrame:\n",
    "        \"\"\"Query all data for reporting with SQL joins returning normalized data\"\"\"\n",
    "        self.logger.info(\"Querying data from all tables...\")\n",
    "\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            query = '''\n",
    "            SELECT\n",
    "                f.firm_crd_nb,\n",
    "                f.sec_nb,\n",
    "                f.business_name,\n",
    "                f.full_legal_name,\n",
    "                f.address,\n",
    "                f.phone_number,\n",
    "                f.employee_count,\n",
    "                f.signatory,\n",
    "                f.created_at,\n",
    "                f.updated_at,\n",
    "                ca.arrangement,\n",
    "                ct.client_type,\n",
    "                ct.aum_value,\n",
    "                pf.fund_name,\n",
    "                pf.fund_id\n",
    "            FROM firms f\n",
    "            LEFT JOIN compensation_arrangements ca ON f.firm_crd_nb = ca.firm_crd_nb\n",
    "            LEFT JOIN client_types ct ON f.firm_crd_nb = ct.firm_crd_nb\n",
    "            LEFT JOIN private_funds pf ON f.firm_crd_nb = pf.firm_crd_nb\n",
    "            ORDER BY f.firm_crd_nb\n",
    "            '''\n",
    "\n",
    "            result = pd.read_sql_query(query, conn)\n",
    "\n",
    "            # Fill NaN values for string columns only\n",
    "            for col in result.select_dtypes(include=['object']).columns:\n",
    "                result[col].fillna('None', inplace=True)\n",
    "\n",
    "            return result\n",
    "\n",
    "    def fetch_compensation(self, firm_crd_nb: int) -> List[str]:\n",
    "        \"\"\"Fetch compensation arrangements for a firm\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT arrangement FROM compensation_arrangements WHERE firm_crd_nb = ?\", (firm_crd_nb,))\n",
    "            return [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "    def fetch_client_types(self, firm_crd_nb: int) -> Dict[str, int]:\n",
    "        \"\"\"Fetch client types and AUM\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT client_type, aum_value FROM client_types WHERE firm_crd_nb = ?\", (firm_crd_nb,))\n",
    "            return {row[0]: row[1] for row in cursor.fetchall()}\n",
    "\n",
    "    def fetch_private_funds(self, firm_crd_nb: int) -> List[Dict[str, str]]:\n",
    "        \"\"\"Fetch private funds\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT fund_name, fund_id FROM private_funds WHERE firm_crd_nb = ?\", (firm_crd_nb,))\n",
    "            return [{\"name\": row[0], \"identification_number\": row[1]} for row in cursor.fetchall()]\n"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:11:47.082468Z",
     "start_time": "2025-03-23T20:11:47.071459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prepare_tables_for_excel(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split the joined query result into separate tables for Excel output\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame output from joining all tables together\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping sheet names to DataFrames for each table\n",
    "    \"\"\"\n",
    "\n",
    "    if df.empty:\n",
    "        return {\"Empty_Result\": pd.DataFrame()}\n",
    "\n",
    "    # Create separate dataframes for each table\n",
    "    result = {}\n",
    "\n",
    "    # Firms table (main table)\n",
    "    firms_cols = [\n",
    "        'firm_crd_nb', 'sec_nb', 'business_name', 'full_legal_name',\n",
    "        'address', 'phone_number', 'employee_count', 'signatory',\n",
    "        'created_at', 'updated_at'\n",
    "    ]\n",
    "    firms_df = df[firms_cols].drop_duplicates(subset=['firm_crd_nb'])\n",
    "    result['Firms'] = firms_df\n",
    "\n",
    "    # Compensation arrangements table\n",
    "    comp_cols = ['firm_crd_nb', 'business_name', 'arrangement']\n",
    "    comp_df = df[comp_cols].dropna(subset=['arrangement'])\n",
    "    comp_df = comp_df[comp_df['arrangement'] != 'None'].drop_duplicates()\n",
    "    if not comp_df.empty:\n",
    "        result['Compensation_Arrangements'] = comp_df\n",
    "\n",
    "    # Client types table\n",
    "    client_cols = ['firm_crd_nb', 'business_name', 'client_type', 'aum_value']\n",
    "    client_df = df[client_cols].dropna(subset=['client_type'])\n",
    "    client_df = client_df[client_df['client_type'] != 'None'].drop_duplicates()\n",
    "    if not client_df.empty:\n",
    "        result['Client_Types'] = client_df\n",
    "\n",
    "    # Private funds table\n",
    "    fund_cols = ['firm_crd_nb', 'business_name', 'fund_name', 'fund_id']\n",
    "    fund_df = df[fund_cols].dropna(subset=['fund_name'])\n",
    "    fund_df = fund_df[fund_df['fund_name'] != 'None'].drop_duplicates()\n",
    "    if not fund_df.empty:\n",
    "        result['Private_Funds'] = fund_df\n",
    "\n",
    "    # Generate summary table\n",
    "    summary_data = []\n",
    "    for firm_crd in firms_df['firm_crd_nb'].unique():\n",
    "        firm_name = firms_df[firms_df['firm_crd_nb'] == firm_crd]['business_name'].iloc[0]\n",
    "\n",
    "        # Count metrics for this firm\n",
    "        comp_count = len(comp_df[comp_df['firm_crd_nb'] == firm_crd]) if 'Compensation_Arrangements' in result else 0\n",
    "        client_count = len(client_df[client_df['firm_crd_nb'] == firm_crd]) if 'Client_Types' in result else 0\n",
    "        fund_count = len(fund_df[fund_df['firm_crd_nb'] == firm_crd]) if 'Private_Funds' in result else 0\n",
    "\n",
    "        # Calculate total AUM if available\n",
    "        total_aum = 0\n",
    "        if 'Client_Types' in result:\n",
    "            firm_clients = client_df[client_df['firm_crd_nb'] == firm_crd]\n",
    "            if not firm_clients.empty:\n",
    "                # Convert to numeric, coercing errors to NaN, then sum\n",
    "                total_aum = pd.to_numeric(firm_clients['aum_value'], errors='coerce').fillna(0).sum()\n",
    "\n",
    "        summary_data.append({\n",
    "            'firm_crd_nb': firm_crd,\n",
    "            'business_name': firm_name,\n",
    "            'total_arrangements': comp_count,\n",
    "            'total_client_types': client_count,\n",
    "            'total_funds': fund_count,\n",
    "            'total_aum': total_aum\n",
    "        })\n",
    "\n",
    "    if summary_data:\n",
    "        result['Summary'] = pd.DataFrame(summary_data)\n",
    "\n",
    "    return result\n"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:06:44.937269Z",
     "start_time": "2025-03-23T20:06:44.926741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Excel writer for MLP pipeline\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class ExcelWriter:\n",
    "    \"\"\"Excel writer for MLP pipeline that writes tables to separate sheets\"\"\"\n",
    "\n",
    "    def __init__(self, config: RunConfiguration):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def write(self, data_dict: Dict[str, pd.DataFrame]) -> str:\n",
    "        \"\"\"\n",
    "        Write multiple dataframes to separate sheets in an Excel file\n",
    "\n",
    "        Args:\n",
    "            data_dict: Dictionary mapping sheet names to pandas DataFrames\n",
    "\n",
    "        Returns:\n",
    "            Path to the created Excel file\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Writing {len(data_dict)} tables to Excel file...\")\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(self.config.output_dir, exist_ok=True)\n",
    "\n",
    "        # Format timestamp for filename\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_path = os.path.join(self.config.output_dir, f'sec_adv_report_{timestamp}.xlsx')\n",
    "\n",
    "        # Create Excel writer\n",
    "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "            # Process each dataframe\n",
    "            for sheet_name, df in data_dict.items():\n",
    "                # Skip empty dataframes\n",
    "                if df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Format specific columns\n",
    "                df_formatted = self._format_dataframe(df, sheet_name)\n",
    "\n",
    "                # Write to Excel\n",
    "                self.logger.info(f\"Writing sheet: {sheet_name} with {len(df_formatted)} rows\")\n",
    "                df_formatted.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "                # Auto-adjust columns width\n",
    "                self._adjust_column_width(writer, sheet_name, df_formatted)\n",
    "\n",
    "        self.logger.info(f\"Excel report generated at {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "    def _format_dataframe(self, df: pd.DataFrame, sheet_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Format specific columns based on sheet name\"\"\"\n",
    "        df_copy = df.copy()\n",
    "\n",
    "        if sheet_name == 'Client_Types' and 'aum_value' in df_copy.columns:\n",
    "            # Format AUM as currency\n",
    "            df_copy['aum_value'] = df_copy['aum_value'].apply(\n",
    "                lambda x: f\"${int(x):,}\" if pd.notna(x) and x != 'None' else \"$0\"\n",
    "            )\n",
    "\n",
    "        # Format date columns across all tables\n",
    "        for col in df_copy.columns:\n",
    "            if 'date' in col.lower() or col in ('created_at', 'updated_at'):\n",
    "                df_copy[col] = pd.to_datetime(df_copy[col], errors='ignore')\n",
    "                if df_copy[col].dtype.kind == 'M':  # If successfully converted to datetime\n",
    "                    df_copy[col] = df_copy[col].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        return df_copy\n",
    "\n",
    "    def _adjust_column_width(self, writer, sheet_name: str, df: pd.DataFrame):\n",
    "        \"\"\"Adjust column width based on content\"\"\"\n",
    "        worksheet = writer.sheets[sheet_name]\n",
    "        for i, col in enumerate(df.columns):\n",
    "            # Calculate maximum column width\n",
    "            max_length = max(\n",
    "                df[col].astype(str).apply(len).max(),\n",
    "                len(str(col))\n",
    "            ) + 2  # Add a little extra space\n",
    "\n",
    "            # Excel has column width limits\n",
    "            col_letter = chr(65 + i) if i < 26 else chr(64 + i // 26) + chr(65 + i % 26)\n",
    "            worksheet.column_dimensions[col_letter].width = min(max_length, 50)\n"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:03:14.798285Z",
     "start_time": "2025-03-23T20:03:14.271270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, Query\n",
    "\n",
    "\n",
    "class APIService:\n",
    "    \"\"\"FastAPI Service for SEC ADV Data\"\"\"\n",
    "\n",
    "    def __init__(self, config: RunConfiguration):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.db_manager = DatabaseSink(config)\n",
    "        self.app = FastAPI(title=\"SEC ADV Data API\", description=\"API for SEC ADV Data\")\n",
    "        self.setup_routes()\n",
    "\n",
    "    def setup_routes(self):\n",
    "        \"\"\"Setup API routes\"\"\"\n",
    "\n",
    "        @self.app.get(\"/firms\", tags=[\"Firms\"])\n",
    "        async def get_firms(limit: int = Query(10, ge=1, le=100)):\n",
    "            \"\"\"Get all firms\"\"\"\n",
    "            try:\n",
    "                df = self.db_manager.query_all().head(limit)\n",
    "\n",
    "                return df.to_dict(orient=\"records\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error getting firms: {str(e)}\")\n",
    "                raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "        @self.app.get(\"/firms/{firm_crd_nb}\", tags=[\"Firms\"])\n",
    "        async def get_firm(firm_crd_nb: int):\n",
    "            \"\"\"Get a specific firm by CRD number\"\"\"\n",
    "            try:\n",
    "                df = self.db_manager.query_all()\n",
    "                firm = df[df[\"firm_crd_nb\"] == firm_crd_nb]\n",
    "\n",
    "                if firm.empty:\n",
    "                    raise HTTPException(status_code=404, detail=f\"Firm with CRD {firm_crd_nb} not found\")\n",
    "\n",
    "                return firm.iloc[0].to_dict()\n",
    "            except HTTPException as e:\n",
    "                raise e\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error getting firm {firm_crd_nb}: {str(e)}\")\n",
    "                raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "        @self.app.get(\"/compensation/{firm_crd_nb}\", tags=[\"Compensation\"])\n",
    "        async def get_compensation(firm_crd_nb: int):\n",
    "            \"\"\"Fetch compensation arrangements for a firm\"\"\"\n",
    "            try:\n",
    "                data = self.db_manager.fetch_compensation(firm_crd_nb)\n",
    "                if not data:\n",
    "                    return {\"firm_crd_nb\": firm_crd_nb, \"compensation\": [], \"message\": \"No compensation data found.\"}\n",
    "                return {\"firm_crd_nb\": firm_crd_nb, \"compensation\": data}\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error fetching compensation data for {firm_crd_nb}: {str(e)}\")\n",
    "                raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "        @self.app.get(\"/client_types/{firm_crd_nb}\", tags=[\"Client Types\"])\n",
    "        async def get_client_types(firm_crd_nb: int):\n",
    "            \"\"\"Fetch client types and AUM\"\"\"\n",
    "            try:\n",
    "                data = self.db_manager.fetch_client_types(firm_crd_nb)\n",
    "                if not data:\n",
    "                    return {\"firm_crd_nb\": firm_crd_nb, \"client_types\": {}, \"message\": \"No client type data found.\"}\n",
    "                return {\"firm_crd_nb\": firm_crd_nb, \"client_types\": data}\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error fetching client types for {firm_crd_nb}: {str(e)}\")\n",
    "                raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "        @self.app.get(\"/private_funds/{firm_crd_nb}\", tags=[\"Private Funds\"])\n",
    "        async def get_private_funds(firm_crd_nb: int):\n",
    "            \"\"\"Fetch private funds for a firm\"\"\"\n",
    "            try:\n",
    "                data = self.db_manager.fetch_private_funds(firm_crd_nb)\n",
    "                if not data:\n",
    "                    return {\"firm_crd_nb\": firm_crd_nb, \"private_funds\": [], \"message\": \"No private funds found.\"}\n",
    "                return {\"firm_crd_nb\": firm_crd_nb, \"private_funds\": data}\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error fetching private funds for {firm_crd_nb}: {str(e)}\")\n",
    "                raise HTTPException(status_code=500, detail=str(e))\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:09:56.236934Z",
     "start_time": "2025-03-23T20:09:56.231561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "as_of_date = datetime(2025, 3, 17)  # Revised working dat\n",
    "working_dir = os.getcwd()\n",
    "\n",
    "# Define the input, output, and database paths relative to the script directory\n",
    "input_dir = os.path.join(working_dir, 'data', 'input')\n",
    "output_dir = os.path.join(working_dir, 'data', 'output')\n",
    "db_path = os.path.join(working_dir, 'data', 'sec_adv.db')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "config = RunConfiguration(\n",
    "    input_dir=input_dir,\n",
    "    output_dir=output_dir,\n",
    "    firm_crd_numbers=[160882, 160021, 317731],\n",
    "    db_path=db_path,\n",
    "    raise_error=False,\n",
    "    verbose=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:07:52.180957Z",
     "start_time": "2025-03-23T20:07:52.170854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "source = Source(config)\n",
    "pdf_transformer = PdfTransformer(config)\n",
    "fund_analysis_transformer = FundAnalysisTransformer(config)\n",
    "db_manager = DatabaseSink(config)\n",
    "excel_writer = ExcelWriter(config)\n",
    "api_service = APIService(config)\n",
    "logger.info(\"Starting MLP pipeline...\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing database at /Users/kyleloomis/development/kyleloomis/mlp/data/sec_adv.db\n",
      "INFO:__main__:Initializing database at /Users/kyleloomis/development/kyleloomis/mlp/data/sec_adv.db\n",
      "INFO:__main__:Starting MLP pipeline...\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Task:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Download Metadata and PDFs (Estimated Time: 30-45 minutes):\n",
    "Using the starter code provided, download the metadata and PDFs for the following FirmCrdNb values. Use the provided URLs to download the files and save them locally. Implement error handling and logging :\n",
    "* 160882\n",
    "* 160021\n",
    "* 1679500"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:10:08.348511Z",
     "start_time": "2025-03-23T20:10:01.398109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pdf_paths = source.run(as_of_date, config.firm_crd_numbers)\n",
    "pdf_paths"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloading metadata for 03_17_2025\n",
      "INFO:__main__:Transforming SEC ADV metadata...\n",
      "INFO:__main__:Downloading PDF files...\n",
      "INFO:__main__:Downloaded /Users/kyleloomis/development/kyleloomis/mlp/data/input/160882.pdf\n",
      "INFO:__main__:Downloaded /Users/kyleloomis/development/kyleloomis/mlp/data/input/160021.pdf\n",
      "INFO:__main__:Downloaded /Users/kyleloomis/development/kyleloomis/mlp/data/input/317731.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{160882: '/Users/kyleloomis/development/kyleloomis/mlp/data/input/160882.pdf',\n",
       " 160021: '/Users/kyleloomis/development/kyleloomis/mlp/data/input/160021.pdf',\n",
       " 317731: '/Users/kyleloomis/development/kyleloomis/mlp/data/input/317731.pdf'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract and Store Information (Estimated Time: 1-1.5 hours):\n",
    "* Task: Extract specific information from the downloaded PDFs and store it in a local SQLite database.\n",
    "* Details: Extract fields such as FirmCrdNb, SECNb, Business Name, Full Legal Name, Address, Phone Number, Compensation Arrangements, Number of employees performing investment advisory functions, Type of Client and Amount of Regulatory Assets Under Management, Names of Private Fund and Private Fund Identification Number, and Signatory of the PDF.\n",
    "* In practice, you will deal with tens of thousands of files. Your code should systematically parse the text and extract the relevant information, as scalability is an important factor."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:10:37.927677Z",
     "start_time": "2025-03-23T20:10:36.025736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "firm_data = {firm_crd_number: pdf_transformer.transform(firm_crd_number) for firm_crd_number, path in\n",
    "                     pdf_paths.items()}\n",
    "db_manager.write(firm_data)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Transforming PDF content at /Users/kyleloomis/development/kyleloomis/mlp/data/input/160882.pdf...\n",
      "INFO:__main__:Transforming PDF content at /Users/kyleloomis/development/kyleloomis/mlp/data/input/160021.pdf...\n",
      "INFO:__main__:Transforming PDF content at /Users/kyleloomis/development/kyleloomis/mlp/data/input/317731.pdf...\n",
      "INFO:__main__:Storing data for 3 firms\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Transformation and Analysis (Estimated Time: 30-45 minutes):\n",
    "* Task: Perform data transformation and analysis using Pandas.\n",
    "* Details: Clean and transform the extracted data, and perform basic analysis such as identifying the top-performing funds based on specific criteria (e.g., assets under management)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:11:00.005711Z",
     "start_time": "2025-03-23T20:10:59.974933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = db_manager.query_all()\n",
    "analysis = fund_analysis_transformer.transform(df)\n",
    "print(analysis['top_funds'])\n",
    "print(analysis['aum_by_client_type'])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Querying data from all tables...\n",
      "/var/folders/v6/1rdtx7yx2rz_g687s_jskfkh0000gn/T/ipykernel_24945/3053069789.py:249: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  result[col].fillna('None', inplace=True)\n",
      "INFO:__main__:Starting fund analysis transformation...\n",
      "INFO:__main__:Identifying top 10 funds by AUM...\n",
      "INFO:__main__:Analyzing AUM distribution by client type...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   firm_crd_nb                    business_name  \\\n",
      "1       160882  MARBLE BAR ASSET MANAGEMENT LLP   \n",
      "2       160882  MARBLE BAR ASSET MANAGEMENT LLP   \n",
      "3       160882  MARBLE BAR ASSET MANAGEMENT LLP   \n",
      "5       160882  MARBLE BAR ASSET MANAGEMENT LLP   \n",
      "4       160882  MARBLE BAR ASSET MANAGEMENT LLP   \n",
      "6       317731            ARROW CAPITAL PTY LTD   \n",
      "0       160021        THREE BRIDGES CAPITAL, LP   \n",
      "\n",
      "                                           fund_name         fund_id  \\\n",
      "1                              CHELODINA MASTER FUND  805-1338780197   \n",
      "2  EAM LONG-ONLY EMERGING MARKETS MASTER FUND LIM...  805-7389231579   \n",
      "3                                 LEXCOR MASTER FUND  805-8547054532   \n",
      "5                                  NAVAT MASTER FUND  805-5647299307   \n",
      "4                           MNAV CAYMAN FUND LIMITED  805-9579883870   \n",
      "6                                               None            None   \n",
      "0             THREE BRIDGES EUROPE MASTER FUND, LTD.  805-7679842650   \n",
      "\n",
      "   total_firm_aum  \n",
      "1     36750000000  \n",
      "2     36750000000  \n",
      "3     36750000000  \n",
      "5     36750000000  \n",
      "4     36750000000  \n",
      "6      2248415682  \n",
      "0       558020067  \n",
      "                  client_type    aum_value  percentage\n",
      "3  Pooled Investment Vehicles  30699650658       77.61\n",
      "2      Other: MANAGED ACCOUNT   8580000000       21.69\n",
      "1              Other Advisers    196179924        0.50\n",
      "0                Corporations     80605167        0.20\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Excel File (Estimated Time: 15-30 minutes):\n",
    "* Task: Extract the following information from the SQLite database and output it in an **Excel file**. Keep in mind that this excel file will be ultimately used by BD recruiters who are considered non-techinical users:\n",
    "* FirmCrdNb\n",
    "* SECNb\n",
    "* Business Name\n",
    "* Full Legal Name\n",
    "* Address\n",
    "* Phone Number\n",
    "* Compensation Arrangements\n",
    "* Number of employees performing investment advisory functions, including research\n",
    "* Type of Client and Amount of Regulatory Assets Under Management\n",
    "* Names of Private Fund and Private Fund Identification Number\n",
    "* Signatory of the PDF"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:11:53.716095Z",
     "start_time": "2025-03-23T20:11:53.523679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tables = prepare_tables_for_excel(df)\n",
    "excel_writer.write(tables)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Writing 5 tables to Excel file...\n",
      "/var/folders/v6/1rdtx7yx2rz_g687s_jskfkh0000gn/T/ipykernel_24945/2684878969.py:71: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  df_copy[col] = pd.to_datetime(df_copy[col], errors='ignore')\n",
      "/var/folders/v6/1rdtx7yx2rz_g687s_jskfkh0000gn/T/ipykernel_24945/2684878969.py:71: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  df_copy[col] = pd.to_datetime(df_copy[col], errors='ignore')\n",
      "INFO:__main__:Writing sheet: Firms with 3 rows\n",
      "INFO:__main__:Writing sheet: Compensation_Arrangements with 9 rows\n",
      "INFO:__main__:Writing sheet: Client_Types with 6 rows\n",
      "INFO:__main__:Writing sheet: Private_Funds with 6 rows\n",
      "INFO:__main__:Writing sheet: Summary with 3 rows\n",
      "INFO:__main__:Excel report generated at /Users/kyleloomis/development/kyleloomis/mlp/data/output/sec_adv_report_20250323_161153.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/kyleloomis/development/kyleloomis/mlp/data/output/sec_adv_report_20250323_161153.xlsx'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scalability and Performance (Discussion - Estimated Time: 15-30 minutes):\n",
    "* Task: Optimize the data pipeline for performance.\n",
    "* Details: Write a brief explanation of how you would handle scalability and performance issues if the dataset were significantly larger."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I have designed the data pipeline to be broken down into 3 main parts: source, transform, and sink. In order to improve scalability and performance, I would improve each part separately, introducing new frameworks/technologies that enable the pipeline to be horizontally scaled. First, the data could be sourced (downloaded) concurrently using either multi-threading, where each thread would be used to request and save a PDF, or an async request pool, a non-blocking protocol useful for interacting with external systems. Second, the transformation layer could be broken up into multiple pieces. I would leverage better OCR tools for PDF text extraction, such as a fine-tuned version of Mistral's OCR LLM. This enables PDFs to be processed more accurately and concurrently in the cloud. The resulting text blobs could then be extracted concurrently using either a multi-threading approach, or better yet, a distributed framework like Spark. Pandas is limited to in-memory processing, whereas Spark can be deployed on a massive compute cluster to horizontally scale to support the size of the workload. Third, the database is the biggest bottleneck in the sink layer. I would replace SQLite with a cloud data warehouse such as BigQuery or Snowflake. This enables massive scalability for handling terabytes or even petabytes of data. Finally, I would tie together these pieces of technology with an orchestration framework such as Airflow, ensuring that the pipeline runs whenever new data arrives and failures are handled properly.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integration with External Systems (Estimated Time: 1-1.5 hours):\n",
    "* Task: Simulate integration with an external system using Fast APIs.\n",
    "* Details: Write a Python script to fetch additional data from a mock API from your SQLite database using standard Python frameworks and data models. Create three GET and PUSH endpoints and demonstrate data quality checks and validations. Provide two tests for each call. Consider and handle potential edge cases."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:23:24.641358Z",
     "start_time": "2025-03-23T20:23:24.532883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# API and Database tests for MLP pipeline (Notebook Version)\n",
    "import sqlite3\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from fastapi.testclient import TestClient\n",
    "\n",
    "# Create temporary database file\n",
    "temp_db = tempfile.NamedTemporaryFile(suffix=\".db\", delete=False)\n",
    "db_path = temp_db.name\n",
    "\n",
    "# Set up test configuration\n",
    "test_config = RunConfiguration(\n",
    "    firm_crd_numbers=[160882],\n",
    "    db_path=db_path,\n",
    "    input_dir=\"./temp\",\n",
    "    output_dir=\"./temp\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Set up database manager\n",
    "db_manager = DatabaseSink(test_config)\n",
    "\n",
    "# Set up FastAPI client\n",
    "api_service = APIService(test_config)\n",
    "api_client = TestClient(api_service.app)\n",
    "\n",
    "# Populate test database with sample data\n",
    "firm_data = {\n",
    "    160882: {\n",
    "        'firm_crd_nb': 160882,\n",
    "        \"sec_nb\": \"801-12345\",\n",
    "        \"business_name\": \"Test Firm 1\",\n",
    "        \"full_legal_name\": \"Test Firm One LLC\",\n",
    "        \"address\": \"123 Test St, Test City, TS 12345\",\n",
    "        \"phone_number\": \"555-1234\",\n",
    "        \"employee_count\": 50,\n",
    "        \"signatory\": \"John Doe\",\n",
    "        \"compensation_arrangements\": [\"Percentage of AUM\", \"Performance-based fees\"],\n",
    "        \"client_types\": {\"Individuals\": 5000000, \"Corporations\": 10000000},\n",
    "        \"private_funds\": {\n",
    "            \"XYZ Fund\": \"805-123456\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "db_manager.write(firm_data)\n",
    "\n",
    "# Verify that the firm was inserted\n",
    "with sqlite3.connect(db_manager.db_path) as conn:\n",
    "    df = pd.read_sql_query(\"SELECT * FROM firms\", conn)\n",
    "    assert not df.empty, \"Firm data was not written to the database\"\n",
    "    assert \"160882\" in df[\"firm_crd_nb\"].astype(str).values, \"Firm CRD 160882 missing\"\n",
    "\n",
    "# Store test results\n",
    "test_results = {}\n",
    "\n",
    "# --- TEST DATABASE INTEGRITY ---\n",
    "print(\"Testing Database Integrity...\")\n",
    "\n",
    "# Test 1: Ensure required tables exist\n",
    "with sqlite3.connect(db_manager.db_path) as conn:\n",
    "    tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table'\", conn)\n",
    "    required_tables = [\"firms\", \"compensation_arrangements\", \"client_types\", \"private_funds\"]\n",
    "    tables_exist = set(required_tables).issubset(set(tables[\"name\"]))\n",
    "    test_results[\"tables_exist\"] = tables_exist\n",
    "    assert tables_exist, f\"Missing tables. Found: {tables['name'].tolist()}\"\n",
    "    print(\"✓ Required tables exist\")\n",
    "\n",
    "# Test 2: Ensure firms table contains expected data\n",
    "with sqlite3.connect(db_manager.db_path) as conn:\n",
    "    firms = pd.read_sql_query(\"SELECT * FROM firms\", conn)\n",
    "    firm_data_exists = len(firms) == 1 and \"160882\" in firms[\"firm_crd_nb\"].astype(str).values\n",
    "    test_results[\"firm_data_exists\"] = firm_data_exists\n",
    "    assert firm_data_exists, \"Firm data not found in database\"\n",
    "    print(\"✓ Firm data exists\")\n",
    "\n",
    "# Test 3: Ensure compensation arrangements are stored correctly\n",
    "comp_data = db_manager.fetch_compensation(160882)\n",
    "comp_correct = set(comp_data) == {\"Percentage of AUM\", \"Performance-based fees\"}\n",
    "test_results[\"compensation_correct\"] = comp_correct\n",
    "assert comp_correct, f\"Compensation arrangements incorrect. Found: {comp_data}\"\n",
    "print(\"✓ Compensation arrangements correct\")\n",
    "\n",
    "# Test 4: Ensure client types and AUM are correctly stored\n",
    "client_data = db_manager.fetch_client_types(160882)\n",
    "client_correct = client_data == {\"Individuals\": 5000000, \"Corporations\": 10000000}\n",
    "test_results[\"client_types_correct\"] = client_correct\n",
    "assert client_correct, f\"Client types incorrect. Found: {client_data}\"\n",
    "print(\"✓ Client types correct\")\n",
    "\n",
    "# Test 5: Ensure private funds are correctly stored\n",
    "fund_data = db_manager.fetch_private_funds(160882)\n",
    "funds_correct = fund_data == [{\"name\": \"XYZ Fund\", \"identification_number\": \"805-123456\"}]\n",
    "test_results[\"private_funds_correct\"] = funds_correct\n",
    "assert funds_correct, f\"Private funds incorrect. Found: {fund_data}\"\n",
    "print(\"✓ Private funds correct\")\n",
    "\n",
    "# --- TEST API ENDPOINTS ---\n",
    "print(\"\\nTesting API Endpoints...\")\n",
    "\n",
    "# Test 6: Test GET /firms endpoint\n",
    "response = api_client.get(\"/firms\")\n",
    "firms_endpoint = response.status_code == 200 and isinstance(response.json(), list)\n",
    "test_results[\"get_firms\"] = firms_endpoint\n",
    "assert firms_endpoint, f\"GET /firms failed. Status: {response.status_code}, Response: {response.json()}\"\n",
    "print(\"✓ GET /firms endpoint works\")\n",
    "\n",
    "# Test 7: Test GET /firms/{firm_crd_nb} endpoint\n",
    "response = api_client.get(\"/firms/160882\")\n",
    "firm_by_id = response.status_code == 200 and response.json()[\"firm_crd_nb\"] == 160882\n",
    "test_results[\"get_firm_by_id\"] = firm_by_id\n",
    "assert firm_by_id, f\"GET /firms/160882 failed. Status: {response.status_code}, Response: {response.json()}\"\n",
    "print(\"✓ GET /firms/{firm_crd_nb} endpoint works\")\n",
    "\n",
    "# Test 8: Test GET /firms/{firm_crd_nb} with invalid firm\n",
    "response = api_client.get(\"/firms/999999\")\n",
    "nonexistent_firm = response.status_code == 404\n",
    "test_results[\"nonexistent_firm\"] = nonexistent_firm\n",
    "assert nonexistent_firm, f\"GET /firms/999999 should return 404. Got: {response.status_code}\"\n",
    "print(\"✓ GET /firms/{nonexistent_firm} returns 404\")\n",
    "\n",
    "# Test 9: Test GET /compensation/{firm_crd_nb}\n",
    "response = api_client.get(\"/compensation/160882\")\n",
    "compensation_endpoint = (response.status_code == 200 and\n",
    "                         set(response.json()[\"compensation\"]) == {\"Percentage of AUM\", \"Performance-based fees\"})\n",
    "test_results[\"get_compensation\"] = compensation_endpoint\n",
    "assert compensation_endpoint, f\"GET /compensation/160882 failed. Status: {response.status_code}, Response: {response.json()}\"\n",
    "print(\"✓ GET /compensation/{firm_crd_nb} endpoint works\")\n",
    "\n",
    "# Test 10: Test GET /client_types/{firm_crd_nb}\n",
    "response = api_client.get(\"/client_types/160882\")\n",
    "client_types_endpoint = (response.status_code == 200 and\n",
    "                        response.json()[\"client_types\"] == {\"Individuals\": 5000000, \"Corporations\": 10000000})\n",
    "test_results[\"get_client_types\"] = client_types_endpoint\n",
    "assert client_types_endpoint, f\"GET /client_types/160882 failed. Status: {response.status_code}, Response: {response.json()}\"\n",
    "print(\"✓ GET /client_types/{firm_crd_nb} endpoint works\")\n",
    "\n",
    "# Test 11: Test GET /private_funds/{firm_crd_nb}\n",
    "response = api_client.get(\"/private_funds/160882\")\n",
    "private_funds_endpoint = (response.status_code == 200 and\n",
    "                         response.json()[\"private_funds\"] == [{\"name\": \"XYZ Fund\", \"identification_number\": \"805-123456\"}])\n",
    "test_results[\"get_private_funds\"] = private_funds_endpoint\n",
    "assert private_funds_endpoint, f\"GET /private_funds/160882 failed. Status: {response.status_code}, Response: {response.json()}\"\n",
    "print(\"✓ GET /private_funds/{firm_crd_nb} endpoint works\")\n",
    "\n",
    "# --- TEST API RESPONSE VALIDATION ---\n",
    "print(\"\\nTesting API Response Validation...\")\n",
    "\n",
    "# Test 12: Ensure firm data response structure matches expected format\n",
    "response = api_client.get(\"/firms/160882\")\n",
    "firm_data = response.json()\n",
    "expected_fields = {\n",
    "    \"firm_crd_nb\",\n",
    "    \"sec_nb\",\n",
    "    \"business_name\",\n",
    "    \"full_legal_name\",\n",
    "    \"address\",\n",
    "    \"phone_number\",\n",
    "    \"employee_count\",\n",
    "    \"signatory\",\n",
    "}\n",
    "firm_format_valid = response.status_code == 200 and expected_fields.issubset(firm_data.keys())\n",
    "test_results[\"firm_format\"] = firm_format_valid\n",
    "assert firm_format_valid, f\"Firm response format invalid. Missing fields: {expected_fields - set(firm_data.keys())}\"\n",
    "print(\"✓ Firm response format is valid\")\n",
    "\n",
    "# Test 13: Ensure compensation response structure is valid\n",
    "response = api_client.get(\"/compensation/160882\")\n",
    "comp_format_valid = (response.status_code == 200 and\n",
    "                     \"compensation\" in response.json() and\n",
    "                     isinstance(response.json()[\"compensation\"], list))\n",
    "test_results[\"compensation_format\"] = comp_format_valid\n",
    "assert comp_format_valid, f\"Compensation format invalid. Response: {response.json()}\"\n",
    "print(\"✓ Compensation response format is valid\")\n",
    "\n",
    "# Test 14: Ensure client types response format is correct\n",
    "response = api_client.get(\"/client_types/160882\")\n",
    "client_format_valid = (response.status_code == 200 and\n",
    "                      \"client_types\" in response.json() and\n",
    "                      isinstance(response.json()[\"client_types\"], dict))\n",
    "test_results[\"client_types_format\"] = client_format_valid\n",
    "assert client_format_valid, f\"Client types format invalid. Response: {response.json()}\"\n",
    "print(\"✓ Client types response format is valid\")\n",
    "\n",
    "# Test 15: Ensure private funds response format is valid\n",
    "response = api_client.get(\"/private_funds/160882\")\n",
    "resp_json = response.json()\n",
    "funds_format_valid = (response.status_code == 200 and\n",
    "                     \"private_funds\" in resp_json and\n",
    "                     isinstance(resp_json[\"private_funds\"], list) and\n",
    "                     \"name\" in resp_json[\"private_funds\"][0] and\n",
    "                     \"identification_number\" in resp_json[\"private_funds\"][0])\n",
    "test_results[\"private_funds_format\"] = funds_format_valid\n",
    "assert funds_format_valid, f\"Private funds format invalid. Response: {resp_json}\"\n",
    "print(\"✓ Private funds response format is valid\")\n",
    "\n",
    "# Clean up\n",
    "temp_db.close()\n",
    "\n",
    "# Summary\n",
    "all_passed = all(test_results.values())\n",
    "print(f\"\\nTest Summary: {'All tests passed!' if all_passed else 'Some tests failed!'}\")\n",
    "print(f\"Passed: {sum(test_results.values())}/{len(test_results)}\")\n",
    "\n",
    "if not all_passed:\n",
    "    failed_tests = [test for test, result in test_results.items() if not result]\n",
    "    print(f\"Failed tests: {failed_tests}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing database at /var/folders/v6/1rdtx7yx2rz_g687s_jskfkh0000gn/T/tmp2qvbu6lj.db\n",
      "INFO:__main__:Initializing database at /var/folders/v6/1rdtx7yx2rz_g687s_jskfkh0000gn/T/tmp2qvbu6lj.db\n",
      "INFO:__main__:Storing data for 1 firms\n",
      "INFO:__main__:Querying data from all tables...\n",
      "/var/folders/v6/1rdtx7yx2rz_g687s_jskfkh0000gn/T/ipykernel_24945/3053069789.py:249: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  result[col].fillna('None', inplace=True)\n",
      "INFO:httpx:HTTP Request: GET http://testserver/firms \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Querying data from all tables...\n",
      "/var/folders/v6/1rdtx7yx2rz_g687s_jskfkh0000gn/T/ipykernel_24945/3053069789.py:249: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  result[col].fillna('None', inplace=True)\n",
      "INFO:httpx:HTTP Request: GET http://testserver/firms/160882 \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Querying data from all tables...\n",
      "/var/folders/v6/1rdtx7yx2rz_g687s_jskfkh0000gn/T/ipykernel_24945/3053069789.py:249: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  result[col].fillna('None', inplace=True)\n",
      "INFO:httpx:HTTP Request: GET http://testserver/firms/999999 \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: GET http://testserver/compensation/160882 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://testserver/client_types/160882 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://testserver/private_funds/160882 \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Querying data from all tables...\n",
      "/var/folders/v6/1rdtx7yx2rz_g687s_jskfkh0000gn/T/ipykernel_24945/3053069789.py:249: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  result[col].fillna('None', inplace=True)\n",
      "INFO:httpx:HTTP Request: GET http://testserver/firms/160882 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://testserver/compensation/160882 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://testserver/client_types/160882 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://testserver/private_funds/160882 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Database Integrity...\n",
      "✓ Required tables exist\n",
      "✓ Firm data exists\n",
      "✓ Compensation arrangements correct\n",
      "✓ Client types correct\n",
      "✓ Private funds correct\n",
      "\n",
      "Testing API Endpoints...\n",
      "✓ GET /firms endpoint works\n",
      "✓ GET /firms/{firm_crd_nb} endpoint works\n",
      "✓ GET /firms/{nonexistent_firm} returns 404\n",
      "✓ GET /compensation/{firm_crd_nb} endpoint works\n",
      "✓ GET /client_types/{firm_crd_nb} endpoint works\n",
      "✓ GET /private_funds/{firm_crd_nb} endpoint works\n",
      "\n",
      "Testing API Response Validation...\n",
      "✓ Firm response format is valid\n",
      "✓ Compensation response format is valid\n",
      "✓ Client types response format is valid\n",
      "✓ Private funds response format is valid\n",
      "\n",
      "Test Summary: All tests passed!\n",
      "Passed: 15/15\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:24:10.301267Z",
     "start_time": "2025-03-23T20:23:40.687933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(api_service.app, host=\"0.0.0.0\", port=9999)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [24945]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:9999 (Press CTRL+C to quit)\n",
      "INFO:__main__:Querying data from all tables...\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-67' coro=<Server.serve() done, defined at /Users/kyleloomis/development/kyleloomis/mlp/venv/lib/python3.10/site-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kyleloomis/development/kyleloomis/mlp/venv/lib/python3.10/site-packages/uvicorn/main.py\", line 579, in run\n",
      "    server.run()\n",
      "  File \"/Users/kyleloomis/development/kyleloomis/mlp/venv/lib/python3.10/site-packages/uvicorn/server.py\", line 66, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "  File \"/Users/kyleloomis/development/kyleloomis/mlp/venv/lib/python3.10/site-packages/nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "  File \"/Users/kyleloomis/development/kyleloomis/mlp/venv/lib/python3.10/site-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/Users/kyleloomis/development/kyleloomis/mlp/venv/lib/python3.10/site-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py\", line 315, in __wakeup\n",
      "    self.__step()\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/Users/kyleloomis/development/kyleloomis/mlp/venv/lib/python3.10/site-packages/uvicorn/server.py\", line 69, in serve\n",
      "    with self.capture_signals():\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py\", line 142, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/Users/kyleloomis/development/kyleloomis/mlp/venv/lib/python3.10/site-packages/uvicorn/server.py\", line 330, in capture_signals\n",
      "    signal.raise_signal(captured_signal)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52687 - \"GET /firms HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/1rdtx7yx2rz_g687s_jskfkh0000gn/T/ipykernel_24945/3053069789.py:249: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  result[col].fillna('None', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52687 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [24945]\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Automated Testing and Data Quality (Discussion - Estimated Time: 15-30 minutes):\n",
    "* Task: Write a brief explanation of how you would implement automated testing and ensure data quality"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For automated testing, I would use pytest to create unit tests for individual components, integration tests for interactions between pipeline stages, and end-to-end tests simulating the full workflow. These tests would run automatically in a CI/CD pipeline. For data quality, I would implement validation at multiple stages: schema validation using Pydantic to enforce data structure, content validation to check for inconsistencies or errors, and statistical validation to identify outliers or anomalous patterns. For monitoring, I would implement dashboards to track key quality metrics and data lineage, with automated alerting for any quality issues. Finally, I would document data quality SLAs and regularly review quality metrics to ensure continuous improvement of both the pipeline and the testing framework.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Identify Top Performing Funds (Discussion - Estimated Time: 15-30 minutes):\n",
    "* Describe how you would use the information above to identify top-performing funds.\n",
    "* List any questions you would ask regarding the task.\n",
    "* Specify any additional information you would need."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To identify top-performing funds, I would enhance the existing database to incorporate risk-adjusted return metrics beyond AUM, such as Sharpe ratio. The Sharpe Ratio is more useful since it indicates the reward per unit of risk taken with the strategy. The max drawdown since inception would also be useful to asses the worst historical performance.\n",
    "Questions:\n",
    "* How long has the fund been around?\n",
    "* What is the Sharpe ratio over the life period of the fund?\n",
    "* What is the benchmark (e.g. index) to compare the fund to?\n",
    "* Has the fund maintained consistent performance through different market conditions?\n",
    "* What is the maximum drawdown the fund has experienced?\n",
    "Additional data: historical returns (ideally monthly), volatility metrics, benchmark performance data, and information about market conditions during the evaluation period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Visualization (Optional - Estimated Time: 30-45 minutes (if included)):\n",
    "* Task: Create visualizations using a Python library such as Matplotlib or Seaborn.\n",
    "* Details: Generate plots to visualize key metrics and insights from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [NOTE] Please do not spend more than 6 hours on this task. Also, provide the time it took you to complete it."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TOTAL TIME FOR COMPLETION: ~6 hours"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
